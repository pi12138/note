# 1.参考资料
	- python网络数据采集
	- 精通Python爬虫框架Scrapy

	- 前置知识：
		- url
		- http协议
		- web前端，html，css，js
		- ajax
		- re，xpath，xml

# 2.爬虫简介
	- 爬虫定义：
	- 两大特征：
		- 能按使用者要求下载数据或者内容
		- 能自动在网络上流窜
	- 三大工作步骤：
		- 下载网页
		- 提取需要的信息
		- 根据一定的规则跳转到另外的网页上执行上两步内容

	- 爬虫分类
		- 通用爬虫
		- 专用爬虫(聚焦爬虫)
	- Python 网络包简介
		- Python2.x：urllib, urllib2, urllib3, httplib, httplib2, requests
		- Python3.x：urllib, urllib3, httplib2, requests
		- Python2：urllib和urllib2配合使用，或者使用requests
		- Python3：urllib和requests

# 3.urllib
	- urllib.request:打开和读取urls
	- urllib.error:包含urllib.request产生的常见错误，使用try捕捉
	- urllib.parse:包含解析url的方法
	- urllib.robotparse:解析robots.txt文件
	- 案例01.py

	- 网页编码问题解决
		- 第三方包chardet：可以自动检测页面文件的编码格式，但是可能有误
		- 案例见02.py

	- urlopen()返回请求对象的url
		- 案例见03.py
		- 内置函数
			- geturl():返回请求对象的url
			- info():请求反馈对象的meta信息
			- getcode():返回http code

	- request.data 的使用
		- 访问网络的两种方式
			- get：
				- 利用参数给服务器传递信息，
				- 参数为字典形式，利用parse.urlencode()编码
				- 案例04.py
			- post：
				- 一般向服务器传递参数使用
				- post是把信息自动加密处理
				- 我们如果想使用post信息，需要用到data参数
				- 使用post意味着http的请求可能需要更改：
					- Content-Type: application/x-www.form-urlencode
					- Content-Length: 数据长度
					- 简而言之，一旦更改请求方法，请注意其他请求头部信息想适应
			- urllib.parse.urlencode()可以字符串自动转换成上面的格式
			- 案例05.py
			- 为了更多的设置请求信息，单纯的通过urlopen函数已经不太好用了
				- 需要利用request.Request 类
				- 案例06.py

	- urllib.error
		- URLError 的产生原因：
			- 没网络
			- 服务器链接失败
			- 找不到指定服务器
			- 是OSError 的一个子类
			- 案例07.py

		- HTTPError是URLError的子类
			- 案例08.py

		- 两者区别：
			- HTTPError是对应的HTTP请求的返回码错误，如果返回错误码是400以上的则引发HTTPError
			- URLError对应的一般是网络出现问题，包括url问题
			- 关系区别：OSError - URLError - HTTPError

	- UserAgent
		- UserAgent:用户代理，简称UA，属于headers的一部分，服务器通过UA来判断访问者身份
		- 常见的UA值，使用的时候可以复制粘贴，也可以使用浏览器访问的时候抓包

		- 设置UA可以使用两种方式
			- headers
			- add_header()
			- 案例09.py


	- ProxyHandler 处理(代理服务器)
		- 使用代理IP，是爬虫的常用手段
		- 获取代理服务器的地址：
			- www.xicidaili.com
			- www.goubanjia.com
		- 代理用来隐藏真实访问中，代理也不能允许频繁的访问某个固定的网址，所以，一定需要很多个代理
		- 基本使用步骤：
			- 1.设置代理地址
			- 2.创建ProxyHandler
			- 3.创建Opener
			- 4.安装Opener
			- 案例10.py
		
	- cookie & session
		- 由于http协议的无记忆性，人们为了弥补这个缺憾，所采用的一个补充协议
		- cookie是发放给用户(即http浏览器)的一段信息
		- session是保存在服务器上的对应的另一半信息，用来记录用户信息
		- 两者的区别：
			- 存放位置不同(cookie存放在用户这,session存放在浏览器上)
			- cookie不安全
			- session会保存在服务器上一段时间，会过期
			- 单个cookie保存的数据不超过4K，很多浏览器限制一个站点最多保存20个
			- 一般情况下cookie只作为身份鉴别，大多信息保存在session上
		- session 的存放位置
			- 存在服务器端
			- 一般情况下，session是放在内存中或者数据库中(一般情况下不需要人为干预，会由框架来自动处理)
		- 没有使用cookie登录
			- 案例new_11.py，不能成功登录到想要登录的页面，会跳转到另一个页面
			- 11.py是爬取登录后的qq邮箱，爬取失败？？？
			- v11.py是爬取qq邮箱的登录页面
		- 使用cookie登录
			- 直接把cookie复制下来，然后手动放入headers
			- 案例12.py
			- http模块包含一些关于cookie的模块，通过使用我们可以自动使用cookie
				- CookieJar
					- 管理存储cookie，向传出的http请求添加cookie
					- cookie存储在内存中，CookieJar实例回收后cookie将消失
				- FileCookieJar(filename, delayload = None, policy = None)：
					- 使用文件管理cookie
					- filename 是保存cookie的文件
				- MozillaCookieJar(filename, delayload = None, policy = None):
					- 创建与Mozilla浏览器cookie.txt兼容的FileCookieJar实例
				- LwpCookieJar(filename, delayload = None, policy = None):
					- 创建与libwww-perl标准兼容的Set-Cookie3格式的FileCookieJar实例
				- 他们的关系：CookieJar --> FileCookieJar --> MozillaCookieJar & LwpCookieJar
		- 利用CookieJar 自动访问人人网
			- 案例13.py
			- 大致流程：
				- 1.打开登录页面后自动通过用户名密码登录
				- 2.自动提取反馈回来的cookie
				- 3.利用提取的cookie登录个人页面
			- handler 是Handler的实例，常用的有
				- 用来处理复杂请求
						```
						# 生成cookie的管理器
						cookie_handler = request.HTTPCookieProcessor(cookie)

						# 创建http请求管理器
						http_handler = request.HTTPHandler()

						# 生成https管理器
						https_handler = request.HTTPSHandler()

						# 创建请求管理器
						opener = request.build_opener(http_handler, https_handler, cookie_handler)
                        ```
			- 创立handler后，使用opener打开，打开后相应的业务应该由handler处理
			- cookie作为变量打印出来，案例14.py
				- cookie的属性
					- name:名称
					- value:值
					- domain:可以访问此cookie的域名
					- path:可以访问此cookie的页面路径
					- expires:过期时间
					- size:大小
					- Http字段
			- 将cookie内容保存到文件， 案例15.py
			- cookie 的读取， 案例16.py
	
	- SSL
		- SSL证书就是指遵守SSL安全套阶层协议的服务器数字证书(SercureSocketLayer)
		- 美国网景公司开发
		- CA(CertifacateAuthority)是数字证书认证中心，是发放，管理，废除数字证书的收信人的第三方机构
		- 遇到不信任的SSL证书，需要单独处理，案例17.py

# 4. js加密 ！！！
	- 有的反爬虫测略采用js对需要传输的数据进行加密处理(通常是取md5值)
	- 经过加密，传输的就是密文
	- 但是加密函数或者过程一定是在浏览器完成，也就是一定会把代码(js代码)暴露给使用者
	- 通过阅读加密算法，就可以模拟出加密过程，从而达到破解
	- 过程参考案例18.py(未成功爬取)
	- 19.py(成功爬取有道词典)，存在问题
	- 爬取有道参考文章
		- https://blog.csdn.net/nunchakushuang/article/details/75294947
		- https://blog.csdn.net/ISxiancai/article/details/79349184
	- 代码格式化：
		- http://tool.chinaz.com/Tools/jsformat.aspx
		- 有道翻译(19.py).txt

# 5. ajax
	- 异步请求
	- 一定会有url请求方法，可能会有数据
	- 一般使用json格式
	- 爬取豆瓣，案例见20.py
	- 进阶版 v20.py


# 6.requests 
	- HTTP for Humans, 更简洁，更友好
	- 继承了urllib的所有特征
	- 底层使用的是urllib3
	- 中文文档：http://docs.python-requests.org/zh_CN/latest/index.html
	- 开源地址：https://github.com/requests/requests
	
	- get请求：
		- requests.get(url)
		- requests.request('get', url)
		- 可以带有headers和parmas参数
		- 案例21.py
	- get返回内容
		r = requests.get(url)
		r.headers
		#http响应内容的头部内容，来返回get请求获得网页的头部信息。
		r.status_code
		#http请求的返回状态，200表示连接成功，404表示连接失败
		r.text
		#http响应内容的字符串形式，url对应的页面内容
		r.encoding
		#从HTTP header中猜测的响应内容编码方式
		r.apparent_encoding
		#从内容分析出的响应内容的编码方式（备选编码方式）
		r.content
		#HTTP响应内容的二进制形式
		- 案例22.py
	
	- post
		- rsp = requests.post(url, data=data)
		- 案例见23.py
		- data 和 headers 要求dict类型
	
	- proxy(代理)
		- 案例24.py
		```
			# 设置代理
			proxies = {
				'http': 'address of proxy',
				'https': 'address of proxy'
			}

			# 使用代理
			rsp = requests.request('get', 'http:xxxxx', proxies = proxies)

		```
		- 代理可能报错，如果使用人数太多，考虑到安全问题，代理会被强行可能关闭

	- 用户验证问题
		- 代理验证
			```python
				# 可能需要使用HTTP basic Auth， 可以这样
				# 格式为："用户名: 密码@代理地址: 端口地址"
				proxy = {'http': 'china: 123456@192.168.1.1:8080'}
				rsp =request.get(url, proxies = proxy)
			```
		
		- web客户端验证
			- 如果遇到web客户端验证，需要添加auth=(用户名，密码)
				```python 
					# 授权信息
					auth = {'test1', '123456'}
					rsp = requests.get(url, auth = auth)
				
				```
	
	- cookie
		- requests 可以自动处理cookie信息
		
		```python 
			rsp = requests.get('http://xxxxxxxx')
			# 如果对方服务器给传送过来cookie信息，则可以通过反馈的cookie属性得到
			# 返回一个cookiejar实例
			cookiejar = rsp.cookies()

			# 可以将cookiejar转换为字典
			cookiedict = requests.utils.dict_from_cookiejar(cookiejar)
		
		```
		- 案例25.py，手动复制cookie登录人人网个人主页
		- 案例26.py，自动复制cookie登录人人网个人主页(用法参考下面的session)
	 
	- session
		- 跟服务端的session不是一个东西(给客户端使用的，不是给服务器使用的)
		- 模拟一次会话，从客户端浏览器链接服务器开始，到客户端浏览器断开
		- 能让我们跨请求时保持某些参数，比如在同一个session实例发出的所有请求之间保持cookie

		```python 
			# 创建session对象，可以保持cookie值
			ss = requests.session()

			headers = {
				'User-Agent': 'xxxxxx'
			}

			data = {'name': 'xxx'}

			# 此时由创建的session管理请求，负责发出请求
			ss.post('http://xxxx', data = data, headers = headers)

			rsp = ss.get("xxxxx")

		```

	- https请求验证ssl证书
		- 参数verity负责表示是否需要验证ssl证书，默认是True
		- 如果不需要验证ssl证书，则设置成False，表示关闭

			```python 
				rsp = requests.get('https://www.baidu.com', verity = false)

			```


		
